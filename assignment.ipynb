{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d44f214",
   "metadata": {},
   "source": [
    "# Programming for Data Analysis Assigment\n",
    "\n",
    "## Initial file creation, assignment to be specified at a later date\n",
    "\n",
    "\"For this project you must create a dataset by simulating a real-world phenomenon of your chooising. You may pick any phenomenon you wish - you might pick on that is of interest to you in your personal or professional life. Then, rather than collect data related to the phenomenon, you should model and synthesise such data using Python. We suggest you use the `numpy.random` package for this purpose.\n",
    "\n",
    "Specificailly, in this project you should:\n",
    "\n",
    "1. Choose a real-world phenomenon that can be measured and for which you could collect at least one-hundred data points across at least four different variables;\n",
    "\n",
    "2. Investigate the types of variables involved, their likely distributions, and their relationships with each other;\n",
    "\n",
    "3. Synthesise/simulate a dataset as closely matching their properties as possible;\n",
    "\n",
    "4. Detail your research and implement the simulation in a Jupyter notebook - the dataset itself can simply be display in an output cell within the notebook.\n",
    "\n",
    "Note that this project is about simulation - you must synthesise a dataset. Some students may already have some real-world datasets in their own files. It is okay to base your synthesised dataset on these should you wish (please reference it if you do), but the main task is in this project is to create a synthesised dataset.\"\n",
    "\n",
    "For point two, it is not enough that you end up with a totally random dataset, you need to see which distribution might be most appropriate for that particular item. What are the likely data points that you collect from a phenomenon? How are the variables related to each other? What does the distribution look like? Is it a normal distribution? A Poisson distribution?\n",
    "\n",
    "It is sufficient for the dataset to be a pandas dataframe, it doesn't need to be a CSV or XML file.\n",
    "\n",
    "Brian: \"So I'm not actually that interested in the actual data. What we're most interested in is the research investigations you do to create the dataset, because presumably there will be some random element in the generation of your data, following some sort of distribution. And I'll be able to re-run your notebook and get a different randomly-generated dataset with the same properties.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bc8fb",
   "metadata": {},
   "source": [
    "## Basic Plan\n",
    "\n",
    "So you need to find a basic dataset that can be extrapolated, rather than something comprehensive. The starting points should probably be either Hitting Against the Spin or the Vollman hockey book. This could be something team-based or player-based, something that can be condensed into small enough chunks that it fits into a 100 row by 5 column (for the four variables) grid.\n",
    "\n",
    "Remember that Brian's example was that he thinks there's a correlation between a student's grade and their level of degree, the amount of time spent studying and the number of commits they make.\n",
    "\n",
    "Could even be the amount of wins gained from winning the toss? Variables would be: toss result, choice of batting/fielding first, match result, total runs in the match. There's ten test teams so the last ten or fifteen results could be handy. Could be: there's a lot of noise about Austin Matthews' 5-on-5 play vs. his shots and likewise his PP goals/shots, how will he do against teams if we can predict their 5-on-5 play and their PK? Variables could be: SOG EV, SOG PP, Goals EV, Goals PP, Opp PK % (for quickness I'd use their overall [season performance](https://www.nhl.com/stats/teams?reportType=season&seasonFrom=20212022&seasonTo=20222023&gameType=2&filter=gamesPlayed,gte,1&sort=points,wins&page=0&pageSize=50), rather than their season-to-date performance to that point, TOI, etc., his 2021/2022 season stats are [here](https://www.nhl.com/player/auston-matthews-8479318?stats=gamelogs-r-nhl&season=20212022).\n",
    "\n",
    "Vollman definitely tell you where he sources all of his data from, and I'm guessing it will be either NHL.com or hockey-reference.com. HAIS the spin should just be a matter of going to Cricinfo and seeing what we can dig out from their core metrics.\n",
    "\n",
    "After this you'll need to show investigation of each variable, and plot them accordingly using NumPy or Seaborn to see if a distribution can be identified. Once these are identified, even just using [Wikipedia](https://en.wikipedia.org/wiki/Probability_distribution#External_links) or the [NumPy documentation](https://numpy.org/doc/stable/reference/random/generator.html), you'll need to use the latest notebooks from this module to simulate a dataset accordingly, so if you plot, say, expected goals, and you can see that the data is Poisson-distributed, you can use the `poisson` function in NumPy to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b40659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# did you use all of these libraries?\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff14d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
